{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7764808",
   "metadata": {},
   "source": [
    "<h1>Introdução</h1>\n",
    "Uma Rede Neural Artificial (RNA) é um modelo computacional inspirado na dinâmica funcional do cérebro humano. Assim como os neurônios biológicos, as redes neurais artificiais são compostas por unidades chamadas neurônios artificiais, que recebem entradas, realizam cálculos matemáticos e produzem uma saída. Quando o valor calculado ultrapassa determinado limite (threshold), o neurônio é ativado.\n",
    "\n",
    "Essas redes são formadas por camadas de neurônios interconectados, que processam informações de maneira distribuída. A partir das combinações de entradas e dos ajustes dos pesos de cada conexão, as RNAs conseguem aprender padrões complexos e realizar tarefas de previsão e classificação.\n",
    "\n",
    "As redes neurais artificiais são capazes de resolver uma ampla variedade de problemas, como reconhecimento de caracteres manuscritos, detecção facial, processamento de linguagem natural e muitas outras aplicações no campo do Deep Learning.\n",
    "\n",
    "Apesar de seu enorme sucesso prático, a maioria das RNAs funciona como uma “caixa-preta”: sabemos que elas aprendem a resolver os problemas de forma eficaz, mas analisar em detalhe como cada decisão interna é tomada ainda é um desafio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4aa6bc",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "O **perceptron** é o modelo mais simples de rede neural artificial, representando de forma abstrata um único neurônio. Ele recebe um conjunto de entradas (geralmente valores binários), atribui pesos a cada uma delas, soma esses valores e aplica uma **função de ativação** para decidir se deve “disparar” ou não.\n",
    "\n",
    "Matematicamente, para entradas $x_1, x_2, \\dots, x_n$ com respectivos pesos $w_1, w_2, \\dots, w_n$ e um termo de ajuste chamado **bias** ($b$), o perceptron calcula:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "A saída é então determinada pela função de ativação (no perceptron clássico, uma função **degrau**):\n",
    "\n",
    "$$\n",
    "y = f(z) =\n",
    "\\begin{cases}\n",
    "1 & \\text{se } z \\geq 0 \\\\\n",
    "0 & \\text{se } z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Isso significa que o perceptron traça uma **fronteira linear** entre classes: se a soma ponderada ultrapassa o limite, ele ativa (1); caso contrário, não ativa (0).\n",
    "\n",
    "Apesar de simples, o perceptron pode implementar **portas lógicas básicas** como **AND**, **OR** e **NOT**. No entanto, problemas mais complexos, como o **XOR**, **não podem ser resolvidos por um perceptron único**, exigindo redes multicamadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5c1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Produto escalar entre dois vetores.\"\"\"\n",
    "    return sum(vi * wi for vi, wi in zip(v, w))\n",
    "\n",
    "def step_function(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Função degrau clássica do perceptron.\n",
    "    \n",
    "    Retorna:\n",
    "        1.0 se x >= 0\n",
    "        0.0 caso contrário\n",
    "    \"\"\"\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a saída de um perceptron.\n",
    "\n",
    "    Args:\n",
    "        weights (Vector): pesos do perceptron (w1, w2, ..., wn)\n",
    "        bias (float): termo de ajuste (threshold)\n",
    "        x (Vector): vetor de entradas (x1, x2, ..., xn)\n",
    "    \n",
    "    Returns:\n",
    "        float: 1.0 se o perceptron \"disparar\", 0.0 caso contrário\n",
    "    \"\"\"\n",
    "    # soma ponderada + bias\n",
    "    total_input = dot(weights, x) + bias\n",
    "    \n",
    "    # aplicar função de ativação degrau\n",
    "    return step_function(total_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25a98de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] -> 0.0\n",
      "[0, 1] -> 0.0\n",
      "[1, 0] -> 0.0\n",
      "[1, 1] -> 1.0\n"
     ]
    }
   ],
   "source": [
    "# AND\n",
    "weights = [2, 2]\n",
    "bias = -3\n",
    "\n",
    "for x in [[0,0], [0,1], [1,0], [1,1]]:\n",
    "    print(f\"{x} -> {perceptron_output(weights, bias, x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc21983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] -> 0.0\n",
      "[0, 1] -> 1.0\n",
      "[1, 0] -> 1.0\n",
      "[1, 1] -> 1.0\n"
     ]
    }
   ],
   "source": [
    "# or\n",
    "weights = [2, 2]\n",
    "bias = -1\n",
    "\n",
    "for x in [[0,0], [0,1], [1,0], [1,1]]:\n",
    "    print(f\"{x} -> {perceptron_output(weights, bias, x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dd10db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] -> 1.0\n",
      "[1] -> 0.0\n"
     ]
    }
   ],
   "source": [
    "#NOT\n",
    "weigths = [-2]\n",
    "bias = 1\n",
    "\n",
    "for x in [[0], [1]]:\n",
    "    print(f\"{x} -> {perceptron_output(weigths, bias, x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba962c1a",
   "metadata": {},
   "source": [
    "No entanto, alguns problemas não podem ser resolvidos por um único perceptron. \n",
    "\n",
    "Por exemplo, o **XOR (OU exclusivo)**, que deve retornar 1 quando apenas uma das entradas é 1, **não pode ser implementado com um único neurônio**. Isso acontece porque o XOR **não é linearmente separável**, ou seja, não existe uma linha única que consiga separar os casos de saída 0 dos casos de saída 1.\n",
    "\n",
    "Porém, assim como os neurônios naturais, os neurônios artificiais **ganham poder de resolver problemas complexos quando estão conectados em múltiplas camadas**. É dessa forma que surgem as **redes neurais multicamadas (MLPs)**, capazes de aprender padrões mais sofisticados e resolver problemas como o XOR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f91b7",
   "metadata": {},
   "source": [
    "# Redes Neurais Feed-Forward\n",
    "\n",
    "As **Redes Feed-Forward** são uma idealização da topologia do cérebro, sendo formadas por **camadas discretas de neurônios conectados em sequência**.  \n",
    "\n",
    "Em geral, uma rede feed-forward possui:\n",
    "\n",
    "- **Camada de entrada**: recebe as entradas e as transmite sem alterá-las.  \n",
    "- **Uma ou mais camadas ocultas**: recebem as saídas da camada anterior, realizam cálculos e transmitem os resultados para a próxima camada.  \n",
    "- **Camada de saída**: produz as saídas finais do modelo.  \n",
    "\n",
    "Como no perceptron, cada neurônio possui **pesos** correspondentes a cada uma de suas entradas e um **viés (bias)**.  \n",
    "Para simplificar, podemos **inserir o viés no final do vetor de pesos** e atribuir a cada neurônio uma entrada de viés sempre igual a 1.  \n",
    "\n",
    "Além disso, em vez de usar a função degrau (`step_function`), que é discreta e não diferenciável, usamos uma **aproximação suave**: a **função sigmoide**, definida como:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "A função sigmoide **mapa qualquer valor real para o intervalo (0,1)**, permitindo que os neurônios produzam saídas contínuas e diferenciáveis — o que é essencial para o treinamento via **gradiente descendente e backpropagation**.\n",
    "\n",
    "Por que usar a **sigmoid** e não <i>step_funcion</i>? A rede neural precisa da realização de cálculos, para o seu treinamento, e por isso funções suaves como a **sigmoid** são melhores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842caf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    \"\"\"Função sigmoide.\"\"\"\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3e5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # o weights inclui  termo de viés, as entradas incluem um 1\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f22cd0",
   "metadata": {},
   "source": [
    "Partindo dessa função, representamos um neurônio como um vetor de pesos, cujo comprimento é um a mais do que o seu número de entradas (devido ao peso de viés). Então, representamos a rede neural como uma lista de <i>camadas</i> (sem entrada) formadas individualmente por uma lista de neurônios\n",
    "\n",
    "Ou seja, representaremos a rede neural como uma lista (camadas) de listas (neurônios) de vetores (pesos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fca30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feed_forward(neural_network: List[List[Vector]], input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"Alimenta o vetor de entrada na rede neural.\n",
    "       Retorna as saídas de todas as camadas (não só da última).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]  # adicionar o viés\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer] # computa saída para cada neurônio\n",
    "        outputs.append(output)\n",
    "\n",
    "        #Entrada para a próxima camada é a saída da camada atual\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f65bf",
   "metadata": {},
   "source": [
    "Agora podemos criar a porta XOR que não conseguimos com um perceptron. Só precisamos ampliar os pesos para que os neuron_outputs fiquem muito próximos de 0 ou 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5858de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [\n",
    "    [ [20,20,-30], #and\n",
    "      [20,20,-10] ], #or\n",
    "      #camada de saída\n",
    "    [ [-60,60,-30] ]  #neurônio de segunda entrada\n",
    "]\n",
    "\n",
    "assert 0.000 < feed_forward(xor_network, [0,0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [0,1])[-1][0] < 1.001\n",
    "assert 0.999 < feed_forward(xor_network, [1,0])[-1][0] < 1.001\n",
    "assert 0.000 < feed_forward(xor_network, [1,1])[-1][0] < 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada00149",
   "metadata": {},
   "source": [
    "# Retropropagação (Backpropagation)\n",
    "\n",
    "Na prática, **não construímos redes neurais manualmente** (como no exemplo do XOR).  \n",
    "Isso porque, em problemas reais — como reconhecimento de imagens — precisamos de centenas ou até milhares de neurônios, e não conseguimos definir manualmente os pesos de cada um deles.\n",
    "\n",
    "Por isso, as redes neurais são **treinadas a partir de dados**.  \n",
    "O algoritmo mais utilizado para isso é a **retropropagação (backpropagation)**, que aplica o **gradiente descendente** (ou suas variantes, como Adam, RMSProp etc.) para ajustar os pesos da rede.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuição do algoritmo\n",
    "\n",
    "Imagine um conjunto de treinamento formado por pares de **entrada** e **saída desejada**.  \n",
    "Exemplo: o problema do **XOR**.\n",
    "\n",
    "A rede começa com pesos aleatórios, e esses pesos vão sendo ajustados pelo seguinte processo:\n",
    "\n",
    "1. **Feed-forward**  \n",
    "   Executamos a passagem direta: a entrada percorre todas as camadas da rede até produzir uma saída.\n",
    "\n",
    "2. **Cálculo do erro**  \n",
    "   Como sabemos qual deveria ser a saída correta, calculamos a **função de perda**.  \n",
    "   Exemplo: erro quadrático médio (MSE):  \n",
    "   $$\n",
    "   L = \\frac{1}{2} \\sum_i^n (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "\n",
    "3. **Gradiente na saída**  \n",
    "   Computamos como a perda varia em relação aos pesos dos **neurônios de saída**.  \n",
    "   (Aqui entra o cálculo de derivadas da função de ativação).\n",
    "\n",
    "4. **Retropropagação dos erros**  \n",
    "   Propagamos os gradientes de volta pela rede, ajustando também os pesos dos **neurônios ocultos**.\n",
    "\n",
    "5. **Atualização dos pesos**  \n",
    "   Ajustamos todos os pesos com um passo de **gradiente descendente**:  \n",
    "   $$\n",
    "   w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "   $$  \n",
    "   onde \\(\\eta\\) é a **taxa de aprendizado**.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumindo\n",
    "A retropropagação é basicamente um ciclo:\n",
    "- **propagar para frente** (feed-forward),  \n",
    "- **calcular o erro**,  \n",
    "- **propagar para trás** (backpropagation),  \n",
    "- **ajustar os pesos**.  \n",
    "\n",
    "Esse processo é repetido muitas vezes até que a rede aprenda a mapear corretamente as entradas para as saídas desejadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1866c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]], input_vector: Vector, target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"Quando houver uma rede neural, um vetor de entrada e um vetor de destino, faça uma previsão e compute o gradiente da perda doos erros quadráticos com relação aos pesos do neurônio.\n",
    "    \"\"\"\n",
    "\n",
    "    #passe para frente\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    #gradientes associados às saídas de pré-ativação dos neurônios de saída\n",
    "    output_deltas = [output * (1-output) * (output - target) for output, target in zip (outputs, target_vector)] # derivada da sigmoide vezes o erro\n",
    "\n",
    "    #gradientes associados aos pesos dos neurônios de saída\n",
    "    output_grads = [[output_deltas[i] * hidden_output for hidden_output in hidden_outputs + [1]] # adicionar o viés\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "    \n",
    "    #gradientes associados às saídas de pré-ativação dos neurônios ocultos\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * dot(output_deltas, [n[i] for n in network[-1]]) # derivada da sigmoide vezes o erro\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    #gradientes associados aos pesos dos neurônios ocultos\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]] # adicionar o viés\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "    \n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2d653",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7d7531c",
   "metadata": {},
   "source": [
    "Hora de treinar a nossa rede neural para aprender o XOR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62dfe10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "#dados de treinamento para o XOR\n",
    "xs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "ys = [[0], [1], [1], [0]]  #saídas desejadas\n",
    "\n",
    "#comece com pesos aleatórios\n",
    "network = [ #camada oculta: 2 entradas -> 2 saídas\n",
    "            [[random.random() for _ in range(2+1)],\n",
    "             [random.random() for _ in range(2+1)]],\n",
    "              #camada de saída: 2 entradas -> 1 saída\n",
    "            [[random.random() for _ in range(2+1)]]   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7f326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Dá um passo de gradiente\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    return [v_i - step_size * grad_i for v_i, grad_i in zip(v, gradient)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d256edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\renna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\renna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365853cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando XOR: 100%|██████████| 20000/20000 [00:00<00:00, 23732.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# Dados do XOR\n",
    "xs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "ys = [[0],[1],[1],[0]]\n",
    "\n",
    "# Rede inicial: 2 neurônios na camada oculta, 1 na de saída\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "network = [\n",
    "    [ [random.uniform(-1,1) for _ in range(3)],  # neurônio 1 (oculta)\n",
    "      [random.uniform(-1,1) for _ in range(3)] ],# neurônio 2 (oculta)\n",
    "    [ [random.uniform(-1,1) for _ in range(3)] ] # saída\n",
    "]\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.tqdm(range(20000), desc=\"Treinando XOR\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        grads = sqerror_gradients(network, x, y)\n",
    "        network = [[gradient_step(neuron, grad, learning_rate)\n",
    "                   for neuron, grad in zip(layer, layer_grads)]\n",
    "                   for layer, layer_grads in zip(network, grads)]\n",
    "\n",
    "# Testes\n",
    "assert feed_forward(network,[0,0])[-1][0] < 0.01\n",
    "assert feed_forward(network,[0,1])[-1][0] > 0.99\n",
    "assert feed_forward(network,[1,0])[-1][0] > 0.99\n",
    "assert feed_forward(network,[1,1])[-1][0] < 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b03926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos da rede neural treinada:\n",
      "\n",
      "Camada 1:\n",
      "  Neurônio 1: [-5.281163769910824, -5.297755324303967, 7.891906536046263]\n",
      "  Neurônio 2: [-6.814847848206161, -6.8995661206393795, 2.8780398608497513]\n",
      "\n",
      "Camada 2:\n",
      "  Neurônio 1: [11.207481902173496, -11.336295008052474, -5.367409380565893]\n",
      "\n",
      "[0, 0] -> 0.0074\n",
      "[0, 1] -> 0.9923\n",
      "[1, 0] -> 0.9923\n",
      "[1, 1] -> 0.0094\n"
     ]
    }
   ],
   "source": [
    "print(\"Pesos da rede neural treinada:\\n\")\n",
    "\n",
    "for i, layer in enumerate(network):\n",
    "    print(f\"Camada {i+1}:\")\n",
    "    for j, neuron in enumerate(layer):\n",
    "        print(f\"  Neurônio {j+1}: {neuron}\")\n",
    "    print()\n",
    "\n",
    "for x in [[0,0],[0,1],[1,0],[1,1]]:\n",
    "    y = feed_forward(network, x)[-1][0]\n",
    "    print(f\"{x} -> {y:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
