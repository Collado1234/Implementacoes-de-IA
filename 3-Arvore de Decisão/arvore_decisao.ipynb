{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b592e3",
   "metadata": {},
   "source": [
    "# üå≥ Implementando uma Decision Tree do Zero\n",
    "\n",
    "Este notebook mostra a implementa√ß√£o **passo a passo** de uma **√°rvore de decis√£o** em Python puro, sem o uso de bibliotecas prontas (como `scikit-learn`).\n",
    "\n",
    "Nosso objetivo ser√° entender **cada etapa** da constru√ß√£o da √°rvore, desde o c√°lculo da impureza at√© a classifica√ß√£o final, com explica√ß√µes te√≥ricas intercaladas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348fa17",
   "metadata": {},
   "source": [
    "## 1. O que √© uma √Årvore de Decis√£o?\n",
    "\n",
    "Uma **√°rvore de decis√£o** √© um modelo de aprendizado supervisionado que divide os dados em grupos cada vez mais homog√™neos com base em perguntas (condi√ß√µes) sobre os atributos.\n",
    "\n",
    "A ideia √© encontrar, a cada divis√£o (n√≥ da √°rvore), a **melhor condi√ß√£o** que separa as classes de forma mais ‚Äúpura‚Äù.\n",
    "\n",
    "Cada n√≥ faz uma **pergunta** do tipo:\n",
    "\n",
    "> ‚ÄúO atributo X √© menor ou igual a um certo valor?‚Äù\n",
    "\n",
    "E os dados s√£o divididos entre os ramos **verdadeiro (esquerda)** e **falso (direita)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8a5a4",
   "metadata": {},
   "source": [
    "## 2. Estrutura da Classe `DecisionTree`\n",
    "\n",
    "A classe `DecisionTree` ser√° respons√°vel por:\n",
    "- **Treinar** a √°rvore com base nos dados (`fit`);\n",
    "- **Classificar** novas amostras (`predict`);\n",
    "- **Construir recursivamente** os n√≥s da √°rvore;\n",
    "- **Calcular a impureza** (Gini ou Entropia);\n",
    "- **Selecionar** o melhor atributo e ponto de corte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713bdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterio=\"gini\", max_depth=None):\n",
    "        self.criterio = criterio\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87abd99",
   "metadata": {},
   "source": [
    "## 3. Fun√ß√µes de Impureza: Gini e Entropia\n",
    "\n",
    "O objetivo de cada divis√£o √© **aumentar a pureza dos n√≥s filhos**.\n",
    "\n",
    "Para medir a pureza, usamos m√©tricas como:\n",
    "\n",
    "### √çndice de Gini\n",
    "$$\n",
    "Gini = 1 - \\sum_i p_i^2\n",
    "$$\n",
    "\n",
    "Onde $ p_i $ √© a propor√ß√£o de elementos da classe *i* no n√≥.\n",
    "\n",
    "### Entropia\n",
    "$$\n",
    "H = - \\sum_i p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "A entropia mede o **grau de desordem** das classes ‚Äî quanto maior, mais misturado o conjunto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511ee30",
   "metadata": {},
   "source": [
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))  # evitar log(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95333138",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))  # evitar log(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa4cb1",
   "metadata": {},
   "source": [
    "## 4. Dividindo os Dados\n",
    "\n",
    "Cada divis√£o (split) separa os dados com base em um **atributo** e um **valor de corte**.\n",
    "\n",
    "Por exemplo:\n",
    "> ‚ÄúSe `altura <= 1.70`, v√° para a esquerda; sen√£o, v√° para a direita.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf121c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _split(self, X, y, feature_index, threshold):\n",
    "        esquerda = X[:, feature_index] <= threshold \n",
    "        direita = X[:, feature_index] > threshold \n",
    "        return X[esquerda], X[direita], y[esquerda], y[direita]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262336e",
   "metadata": {},
   "source": [
    "## 5. Escolhendo a Melhor Divis√£o\n",
    "\n",
    "Precisamos testar **todas as poss√≠veis divis√µes** e escolher a que **gera o maior ganho de informa√ß√£o** (ou seja, a maior redu√ß√£o da impureza total).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best_split(self, X, y):\n",
    "    # Inicializa a vari√°vel que rastreia o maior ganho de impureza encontrado at√© o momento.\n",
    "    # O objetivo √© maximizar este valor.\n",
    "    melhor_ganho = -1 \n",
    "    \n",
    "    # Inicializa as vari√°veis que armazenar√£o a caracter√≠stica (feature) e o valor de corte (threshold)\n",
    "    # que produzirem o 'melhor_ganho'.\n",
    "    melhor_feature, melhor_threshold = None, None\n",
    "    \n",
    "    # Calcula a impureza do n√≥ atual ANTES de qualquer divis√£o.\n",
    "    # A m√©trica utilizada (Gini ou Entropia) depende do que foi configurado no atributo 'criterio'.\n",
    "    impureza_atual = self._gini(y) if self.criterio == \"gini\" else self._entropy(y)\n",
    "\n",
    "    # Obt√©m o n√∫mero total de caracter√≠sticas (colunas) no conjunto de dados X.\n",
    "    n_features = X.shape[1] \n",
    "\n",
    "    # --- IN√çCIO DA BUSCA PELO MELHOR SPLIT ---\n",
    "    \n",
    "    # Loop principal: Itera sobre cada caracter√≠stica (coluna) do conjunto de dados.\n",
    "    for feature in range(n_features): \n",
    "        # Obt√©m todos os valores √∫nicos presentes na caracter√≠stica atual.\n",
    "        # Estes valores ser√£o testados como poss√≠veis pontos de corte (thresholds).\n",
    "        valores = np.unique(X[:, feature]) \n",
    "        \n",
    "        # Loop interno: Itera sobre cada valor √∫nico como um potencial ponto de corte.\n",
    "        for threshold in valores:\n",
    "            # Chama a fun√ß√£o auxiliar '_split' para dividir o conjunto de dados (X e y)\n",
    "            # usando a 'feature' e o 'threshold' atuais.\n",
    "            # X_esq/y_esq: Dados que satisfazem a condi√ß√£o (valor da feature <= threshold).\n",
    "            # X_dir/y_dir: Dados que N√ÉO satisfazem a condi√ß√£o (valor da feature > threshold).\n",
    "            X_esq, X_dir, y_esq, y_dir = self._split(X, y, feature, threshold)\n",
    "            \n",
    "            # Condi√ß√£o de parada: Se uma das divis√µes resultar em um n√≥ vazio, a divis√£o √© inv√°lida.\n",
    "            # Isso evita a cria√ß√£o de n√≥s filhos vazios, que n√£o contribuem para o ganho.\n",
    "            if len(y_esq) == 0 or len(y_dir) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calcula a propor√ß√£o de exemplos que foram para o n√≥ esquerdo ('esq').\n",
    "            # p √© o peso do n√≥ esquerdo na impureza ponderada.\n",
    "            p = len(y_esq) / len(y)\n",
    "            \n",
    "            # Calcula a impureza do n√≥ esquerdo, usando o mesmo crit√©rio (Gini ou Entropia) do n√≥ pai.\n",
    "            impureza_esq = self._gini(y_esq) if self.criterio == \"gini\" else self._entropy(y_esq)\n",
    "            \n",
    "            # Calcula a impureza do n√≥ direito.\n",
    "            impureza_dir = self._gini(y_dir) if self.criterio == \"gini\" else self._entropy(y_dir)\n",
    "            \n",
    "            # C√°lculo do GANHO DE IMPUREZA: \n",
    "            # √â a diferen√ßa entre a impureza do n√≥ pai e a soma ponderada das impurezas dos n√≥s filhos.\n",
    "            # Um valor maior significa uma redu√ß√£o mais eficaz na impureza.\n",
    "            impureza_ponderada_depois = (p * impureza_esq + (1 - p) * impureza_dir)\n",
    "            ganho = impureza_atual - impureza_ponderada_depois\n",
    "\n",
    "            # --- ATUALIZA√á√ÉO DO MELHOR SPLIT ---\n",
    "            \n",
    "            # Se o ganho da divis√£o atual for maior que o melhor ganho registrado at√© agora,\n",
    "            # armazena esta divis√£o como a melhor.\n",
    "            if ganho > melhor_ganho:\n",
    "                melhor_ganho = ganho\n",
    "                melhor_feature = feature\n",
    "                melhor_threshold = threshold\n",
    "\n",
    "    # Retorna a caracter√≠stica e o ponto de corte que maximizaram o ganho de impureza.\n",
    "    # Estas ser√£o usadas para criar o pr√≥ximo n√≥ de teste na √°rvore.\n",
    "    return melhor_feature, melhor_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f8214",
   "metadata": {},
   "source": [
    "## 6. Construindo a √Årvore Recursivamente\n",
    "\n",
    "A constru√ß√£o da √°rvore √© recursiva:\n",
    "- Se o n√≥ for ‚Äúpuro‚Äù (todas as classes iguais) ou atingir profundidade m√°xima ‚Üí vira **folha**.\n",
    "- Caso contr√°rio ‚Üí encontra o **melhor split**, divide e cria **n√≥s filhos**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _build_tree(self, X, y, depth=0):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        node = {\"depth\": depth, \"samples\": len(y), \"classes\": dict(zip(classes, counts))}\n",
    "\n",
    "        # Caso de parada\n",
    "        if len(classes) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            node[\"type\"] = \"leaf\"\n",
    "            node[\"prediction\"] = classes[np.argmax(counts)]\n",
    "            return node\n",
    "\n",
    "        feature, threshold = self._best_split(X, y)\n",
    "        if feature is None:\n",
    "            node[\"type\"] = \"leaf\"\n",
    "            node[\"prediction\"] = classes[np.argmax(counts)]\n",
    "            return node\n",
    "\n",
    "        node[\"type\"] = \"node\"\n",
    "        node[\"feature\"] = feature\n",
    "        node[\"threshold\"] = threshold\n",
    "\n",
    "        X_esq, X_dir, y_esq, y_dir = self._split(X, y, feature, threshold)\n",
    "        node[\"left\"] = self._build_tree(X_esq, y_esq, depth + 1)\n",
    "        node[\"right\"] = self._build_tree(X_dir, y_dir, depth + 1)\n",
    "\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d2997",
   "metadata": {},
   "source": [
    "## 7. Treinamento e Predi√ß√£o\n",
    "\n",
    "Agora, implementamos os m√©todos principais:\n",
    "- `fit(X, y)`: constr√≥i a √°rvore;\n",
    "- `predict(X)`: percorre a √°rvore para classificar novas amostras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_sample(self, x, node, caminho=None):\n",
    "        if caminho is None:\n",
    "            caminho = []\n",
    "\n",
    "        if node[\"type\"] == \"leaf\":\n",
    "            caminho.append(f\"‚Üí Folha: classe = {node['prediction']}\")\n",
    "            return node[\"prediction\"], caminho\n",
    "\n",
    "        feature = node[\"feature\"]\n",
    "        threshold = node[\"threshold\"]\n",
    "\n",
    "        if x[feature] <= threshold:\n",
    "            caminho.append(f\"Se X[{feature}] <= {threshold}: seguir para esquerda\")\n",
    "            return self._predict_sample(x, node[\"left\"], caminho)\n",
    "        else:\n",
    "            caminho.append(f\"Se X[{feature}] > {threshold}: seguir para direita\")\n",
    "            return self._predict_sample(x, node[\"right\"], caminho)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds, caminhos = [], []\n",
    "        for x in X:\n",
    "            pred, caminho = self._predict_sample(x, self.tree)\n",
    "            preds.append(pred)\n",
    "            caminhos.append(caminho)\n",
    "        return np.array(preds), caminhos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99286c80",
   "metadata": {},
   "source": [
    "## 8. Testando a √Årvore com Dados Simples\n",
    "Vamos testar com um conjunto de dados artificial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a368b451",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTree' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m arvore \u001b[38;5;241m=\u001b[39m DecisionTree(criterio\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43marvore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(X, y)\n\u001b[0;32m     13\u001b[0m preds, caminhos \u001b[38;5;241m=\u001b[39m arvore\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (p, c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(preds, caminhos)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTree' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [2.7, 2.5],\n",
    "    [1.3, 1.5],\n",
    "    [3.0, 3.5],\n",
    "    [2.0, 1.0],\n",
    "    [3.5, 0.5]\n",
    "])\n",
    "y = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "arvore = DecisionTree(criterio=\"gini\", max_depth=3)\n",
    "arvore.fit(X, y)\n",
    "\n",
    "preds, caminhos = arvore.predict(X)\n",
    "for i, (p, c) in enumerate(zip(preds, caminhos)):\n",
    "    print(f\"Exemplo {i}: classe prevista = {p}\")\n",
    "    print(\"  Caminho de decis√£o:\")\n",
    "    for passo in c:\n",
    "        print(\"   \", passo)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424e590",
   "metadata": {},
   "source": [
    "## 9. Conclus√£o\n",
    "\n",
    "Neste notebook, vimos **como uma √°rvore de decis√£o √© constru√≠da do zero**, entendendo passo a passo:\n",
    "\n",
    "- O c√°lculo da **impureza (Gini e Entropia)**;  \n",
    "- O processo de **divis√£o √≥tima dos dados**;  \n",
    "- A **recurs√£o na constru√ß√£o da √°rvore**;  \n",
    "- E, por fim, a **interpreta√ß√£o das decis√µes tomadas**.\n",
    "\n",
    "Esse entendimento √© essencial antes de usar implementa√ß√µes prontas como `DecisionTreeClassifier` do `scikit-learn`, pois nos permite compreender o *porqu√™* das decis√µes do modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
